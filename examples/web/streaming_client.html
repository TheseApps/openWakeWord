<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Websocket Microphone Streaming</title>
  <style>
    body {
      text-align: center;
      font-family: 'Roboto', sans-serif;
    }
    #startButton {
      padding: 15px 30px;
      font-size: 18px;
      background-color: #03A9F4;
      border: none;
      border-radius: 4px;
      color: white;
      cursor: pointer;
      outline: none;
      transition: background-color 0.3s;
    }
    #startButton.listening {
      background-color: #4CAF50;
    }

    table {
      margin: 20px auto;
      border-collapse: collapse;
      width: 60%;
    }
    th, td {
      border: 1px solid #E0E0E0;
      padding: 10px;
      text-align: left;
    }
    th {
      background-color: #F5F5F5;
    }

    @keyframes fadeOut {
      from {
        opacity: 1;
      }
      to {
        opacity: 0;
      }
    }

    .detected-animation {
      animation: fadeOut 2s forwards;
    }
  </style>
</head>
<body>
  <h1>Streaming Audio to openWakeWord Using Websockets</h1>
  <button id="startButton">Start Listening</button>

  <table>
    <tr>
      <th>Wakeword</th>
      <th>Detected</th>
    </tr>
    <tr>
      <td></td>
      <td></td>
    </tr>
  </table>

  <script>
  // Create websocket connection
  const ws = new WebSocket('ws://localhost:9000/ws');

  // When the websocket connection is open
  ws.onopen = function() {
    console.log('WebSocket connection is open');
  };

  // Define mapping from model names to brightness values
  const modelBrightnessMap = {
    'hey_mycroft_v0.1': 20,
    'hey_jarvis_v0.1': 200,
    'alexa_v0.1': 'cookie monster' // Intentionally incorrect value for testing
    // Add other models here if needed
  };

  // Cooldown tracking for API calls (milliseconds)
  const apiCooldown = 2000; // 2 seconds
  let lastApiCallTimestamp = {}; // Store last call time per model

  // Get responses from websocket and display information
  ws.onmessage = (event) => {
    // console.log("Received raw data:", event.data); // Commented out for less noise
    let model_payload;
    try {
        model_payload = JSON.parse(event.data);
    } catch (e) {
        console.error("Failed to parse JSON:", e, "Raw data:", event.data);
        return; // Stop processing if JSON is invalid
    }

    // Log the parsed payload ONLY when a specific model activation occurs (moved below)
    // if ("activations" in model_payload && model_payload.activations.length > 0) {
    //   console.log("Parsed activation payload:", model_payload);
    // }


    if ("loaded_models" in model_payload) {
      // Add loaded models to the rows of the first column in the table, inserting rows as needed
      const table = document.querySelector('table');
      const rows = table.querySelectorAll('tr');
      for (let i = 1; i < model_payload.loaded_models.length + 1; i++) {
        if (i < rows.length) {
          const row = rows[i];
          const cell = row.querySelectorAll('td')[0];
          cell.textContent = model_payload.loaded_models[i - 1];
        } else {
          // Insert extra rows if needed, both column 1 and 2
          const row = table.insertRow();
          const cell1 = row.insertCell();
          const cell2 = row.insertCell();
          cell1.textContent = model_payload.loaded_models[i - 1];
          cell2.textContent = '';
        }
      }
    }

    if ("activations" in model_payload) {
      const table = document.querySelector('table');
      const rows = table.querySelectorAll('tr');
      for (let i = 1; i < rows.length; i++) {
        const modelCell = rows[i].querySelectorAll('td')[0];
        const modelName = modelCell.textContent;

        // Check if this specific model name is in the received activations
        if (model_payload.activations.includes(modelName)) {
          // Log the payload only when a relevant model is activated
          console.log("Parsed activation payload:", model_payload);
          console.log(`Activation detected for model: ${modelName}`); // Log specific model detection

          // --- Cooldown Check --- 
          const now = Date.now();
          if (!lastApiCallTimestamp[modelName] || (now - lastApiCallTimestamp[modelName] > apiCooldown)) {
              console.log(`Cooldown passed for ${modelName}, proceeding with actions.`);
              lastApiCallTimestamp[modelName] = now; // Update timestamp *before* potential async operations

              // Update table cell visual
              const cell = rows[i].querySelectorAll('td')[1];
              cell.textContent = "Detected!";
              cell.classList.add('detected-animation');

              // Remove the CSS class after the fade out animation ends to reset the state
              cell.addEventListener('animationend', () => {
                  cell.textContent = '';
                  cell.classList.remove('detected-animation');
              }, { once: true });

              // *** Call MagicMirror API based on model name (inside cooldown block) ***
              if (modelName in modelBrightnessMap) {
                  const brightnessValue = modelBrightnessMap[modelName];
                  const apiUrl = `http://localhost:8080/api/brightness/${brightnessValue}`;
                  console.log(`Calling API: ${apiUrl}`); // Log the API call being made

                  fetch(apiUrl)
                    .then(response => {
                      console.log(`API Response Status: ${response.status} ${response.statusText}`);
                      if (!response.ok) {
                        // Log error if response status is not OK (e.g., 4xx, 5xx)
                        console.error(`MagicMirror API call failed for ${modelName}. Status: ${response.status}`);
                        // Optionally, try to read the response body for more details
                        response.text().then(text => {
                           console.error("API Error Body:", text);
                        }).catch(err => {
                           console.error("Could not read error response body:", err);
                        });
                      } else {
                        console.log(`MagicMirror API call successful for ${modelName}.`);
                        // Optionally log success body if needed
                        // response.text().then(text => console.log("API Success Body:", text));
                      }
                    })
                    .catch(error => {
                      // Log network errors or other issues with the fetch itself
                      console.error(`Error calling MagicMirror API for ${modelName}:`, error);
                      // Note: Standard Fetch API errors don't typically have an 'innerException' property.
                      // 'error.message', 'error.name', and 'error.stack' (in some envs) are common.
                      if (error.cause) {
                          console.error("Error Cause:", error.cause); // Log cause if available
                      }
                    });
              } else {
                console.log(`No brightness mapping found for detected model: ${modelName}`);
              }
              // *** END API CALL CODE ***
          } else {
               console.log(`Cooldown active for ${modelName}. Skipping actions.`);
          }
          // --- End Cooldown Check ---
        }
      }
    }
  };

  // Create microphone capture stream for 16-bit PCM audio data
  // Code based on the excellent tutorial by Ragy Morkas: https://medium.com/@ragymorkos/gettineg-monochannel-16-bit-signed-integer-pcm-audio-samples-from-the-microphone-in-the-browser-8d4abf81164d
  navigator.getUserMedia = navigator.getUserMedia ||
                         navigator.webkitGetUserMedia ||
                         navigator.mozGetUserMedia ||
                         navigator.msGetUserMedia;

  let audioStream;
  let audioContext;
  let recorder;
  let volume;
  let sampleRate;

  if (navigator.getUserMedia) {
    navigator.getUserMedia({audio: true}, function(stream) {
      audioStream = stream;

      // creates the an instance of audioContext
      const context = window.AudioContext || window.webkitAudioContext;
      audioContext = new context();

      // retrieve the current sample rate of microphone the browser is using and send to Python server
      sampleRate = audioContext.sampleRate;

      // creates a gain node
      volume = audioContext.createGain();

      // creates an audio node from the microphone incoming stream
      const audioInput = audioContext.createMediaStreamSource(audioStream);

      // connect the stream to the gain node
      audioInput.connect(volume);

      const bufferSize = 4096;
      recorder = (audioContext.createScriptProcessor ||
                  audioContext.createJavaScriptNode).call(audioContext,
                                                          bufferSize,
                                                          1,
                                                          1);

      recorder.onaudioprocess = function(event) {
        const samples = event.inputBuffer.getChannelData(0);
        const PCM16iSamples = samples.map(sample => {
          let val = Math.floor(32767 * sample);
          return Math.min(32767, Math.max(-32768, val));
        });

        // Push audio to websocket
        const int16Array = new Int16Array(PCM16iSamples);
        const blob = new Blob([int16Array], { type: 'application/octet-stream' });
        // Ensure websocket is open before sending
        if (ws.readyState === WebSocket.OPEN) {
             ws.send(blob);
        } else {
             console.warn("WebSocket not open. Cannot send audio data.");
        }
      };

    }, function(error) {
      console.error('Error capturing audio.', error); // Log error object
      alert('Error capturing audio. Check console for details.');
    });
  } else {
    alert('getUserMedia not supported in this browser.');
  }

  // start recording
  const startButton = document.getElementById('startButton');
  startButton.addEventListener('click', function() {
    if (!startButton.classList.contains('listening')) {
      if (audioContext && audioContext.state === 'suspended') {
          audioContext.resume(); // Resume audio context if suspended (e.g., by browser policy)
      }
      if (volume && recorder && audioContext) {
           volume.connect(recorder);
           recorder.connect(audioContext.destination);
           if (ws.readyState === WebSocket.OPEN) {
                ws.send(sampleRate); // Send sample rate only when starting
                startButton.classList.add('listening');
                startButton.textContent = 'Listening...';
           } else {
                console.error("WebSocket not open. Cannot start listening.");
                alert("WebSocket connection is not open. Please ensure the server is running and refresh the page.");
           }
      } else {
          console.error("Audio components not initialized properly.");
          alert("Audio components not ready. Please allow microphone access and refresh.");
      }
    }
    // Note: Added no 'stop' functionality here, assuming continuous listening once started.
  });
  </script>
</body>
</html>